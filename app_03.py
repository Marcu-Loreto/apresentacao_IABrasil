# app.py
import os
import streamlit as st
from dotenv import load_dotenv
from streamlit.components.v1 import html as st_html
from openai import OpenAI
import json
from pathlib import Path
import re
from io import BytesIO
from collections import Counter
import base64
import time
import pickle
from datetime import datetime

# Imports opcionais
try:
    from difflib import SequenceMatcher
    _SEQUENCEMATCHER_AVAILABLE = True
except Exception:
    SequenceMatcher = None
    _SEQUENCEMATCHER_AVAILABLE = False

try:
    from wordcloud import WordCloud
    _WORDCLOUD_AVAILABLE = True
except Exception:
    _WORDCLOUD_AVAILABLE = False

try:
    import networkx as nx
    from pyvis.network import Network
    _GRAPH_AVAILABLE = True
except Exception:
    nx = None
    Network = None
    _GRAPH_AVAILABLE = False

try:
    import pandas as pd
    _PANDAS_AVAILABLE = True
except Exception:
    pd = None
    _PANDAS_AVAILABLE = False

# Carrega variÃ¡veis do .env
load_dotenv()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CORRETOR ORTOGRÃFICO INTEGRADO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CORREÃ‡Ã•ES_ORTOGRÃFICAS = {
    # Erros comuns de digitaÃ§Ã£o
    "tbm": "tambÃ©m",
    "vc": "vocÃª",
    "tb": "tambÃ©m",
    "q": "que",
    "eh": "Ã©",
    "mt": "muito",
    "td": "tudo",
    "blz": "beleza",
    "obg": "obrigado",
    "vlw": "valeu",
    "pq": "porque",
    "Ã±": "nÃ£o",
    "oq": "o que",
    "dps": "depois",
    "hj": "hoje",
    "amg": "amigo",
    "msg": "mensagem",
    "msm": "mesmo",
    "cmg": "comigo",
    # Erros de acentuaÃ§Ã£o comuns
    "nao": "nÃ£o",
    "entao": "entÃ£o",
    "voce": "vocÃª",
    "esta": "estÃ¡",
    "ate": "atÃ©",
    "porem": "porÃ©m",
    "tambem": "tambÃ©m",
    "numero": "nÃºmero",
    "telefone": "telefone",
    "codigo": "cÃ³digo",
    "pedido": "pedido",
    "prazo": "prazo",
    "endereco": "endereÃ§o",
    "reclamacao": "reclamaÃ§Ã£o",
    "solucao": "soluÃ§Ã£o",
    "atencao": "atenÃ§Ã£o",
    "informacao": "informaÃ§Ã£o",
}


def corrigir_palavra(palavra: str) -> str:
    """Corrige uma palavra usando dicionÃ¡rio de correÃ§Ãµes."""
    palavra_lower = palavra.lower()
    
    if palavra_lower in CORREÃ‡Ã•ES_ORTOGRÃFICAS:
        correcao = CORREÃ‡Ã•ES_ORTOGRÃFICAS[palavra_lower]
        
        # Preserva capitalizaÃ§Ã£o
        if palavra[0].isupper():
            return correcao.capitalize()
        return correcao
    
    return palavra


def corrigir_texto(texto: str) -> str:
    """Corrige ortografia de um texto completo."""
    tokens = re.findall(r'\b\w+\b|[^\w\s]', texto)
    
    corrigido = []
    for token in tokens:
        if re.match(r'\w+', token):
            corrigido.append(corrigir_palavra(token))
        else:
            corrigido.append(token)
    
    return ' '.join(corrigido)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ASSISTENTE DEFINIDO 100% NO CÃ“DIGO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SYSTEM_PROMPT = """
VocÃª Ã© o Assistente de Atendimento e ConciliaÃ§Ã£o da empresa.
MissÃ£o: resolver solicitaÃ§Ãµes de clientes com rapidez, cordialidade e foco em acordos justos.
VocÃª Ã© um assistente que responde apenas apÃ³s a primeira mensagem do usuÃ¡rio.
NÃ£o peÃ§a nome nem dados pessoais por padrÃ£o.
Se a conversa estiver vazia, nÃ£o diga nada.

PrincÃ­pios:
1) Clareza, objetividade e empatia; trate o cliente pelo nome se fornecido.
2) Confirme entendimento do caso em 1 frase antes de propor soluÃ§Ã£o.
3) Traga opÃ§Ãµes de conciliaÃ§Ã£o: reenvio, abatimento, reembolso (parcial/total), crÃ©dito em conta, cupom.
4) Explique prazos, documentos necessÃ¡rios e prÃ³ximos passos com bullets curtos.
5) Se faltar informaÃ§Ã£o, faÃ§a no mÃ¡ximo 2 perguntas diretas e relevantes.
6) Evite jargÃµes; linguagem simples e educada.
7) Respeite polÃ­ticas: nÃ£o prometa o que nÃ£o pode cumprir; se necessÃ¡rio, escale ao time responsÃ¡vel.
8) ProteÃ§Ã£o de dados: nÃ£o invente dados do cliente; confirme somente o que foi informado.

Formato da resposta:
- Resumo do caso:
- SoluÃ§Ã£o proposta:
- PrÃ³ximos passos:
- ObservaÃ§Ãµes:

Exemplo de tom:
"Entendi o ocorrido e quero resolver isso da forma mais rÃ¡pida e justa para vocÃª."
"""

CONFIG = {
    "modelo_padrao": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
    "modelo_sentimento": os.getenv("OPENAI_SENTIMENT_MODEL", "gpt-4o-mini"),
    "temperatura_padrao": 0.3,
    "max_tokens_padrao": 500,
    "max_contexto_mensagens": 20,
    "max_contexto_rag": 3,
    "sentimento_habilitado": True,
    "correcao_ortografica": True,
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VALIDAÃ‡ÃƒO E CLIENTE OPENAI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("ğŸ”’ OPENAI_API_KEY nÃ£o encontrada. Defina no arquivo .env")
    st.stop()

if not OPENAI_API_KEY.startswith("sk-"):
    st.error("ğŸ”’ OPENAI_API_KEY invÃ¡lida. Deve comeÃ§ar com 'sk-'")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)


def obter_mensagens_completas():
    """Retorna mensagens com janela deslizante para otimizar tokens."""
    max_msgs = CONFIG["max_contexto_mensagens"]
    msgs_usuario = st.session_state.get("lista_mensagens", [])
    
    msgs_recentes = msgs_usuario[-max_msgs:] if len(msgs_usuario) > max_msgs else msgs_usuario
    
    return [{"role": "system", "content": SYSTEM_PROMPT}] + msgs_recentes


def call_llm(
    user_message: str,
    *,
    model: str = None,
    temperature: float = None,
    max_tokens: int = None,
) -> str:
    """Chamada robusta Ã  API OpenAI com parÃ¢metros configurÃ¡veis."""
    model = model or CONFIG["modelo_padrao"]
    temperature = temperature if temperature is not None else CONFIG["temperatura_padrao"]
    max_tokens = max_tokens or CONFIG["max_tokens_padrao"]
    
    messages = obter_mensagens_completas()
    messages.append({"role": "user", "content": user_message})
    
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        st.error(f"âŒ Erro na API OpenAI: {str(e)}")
        return f"Desculpe, ocorreu um erro ao processar sua mensagem: {str(e)}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANÃLISE DE SENTIMENTO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _formatar_prompt_sentimento(texto: str) -> str:
    return (
        "VocÃª Ã© um classificador de sentimento. Classifique a mensagem a seguir.\n"
        "Responda APENAS com JSON vÃ¡lido com as chaves exatamente assim:\n"
        '{"label":"positivo|neutro|negativo","confidence":0.0-1.0,"emotions":["..."],"reason":"..."}\n'
        "Mensagem:\n"
        f"{texto.strip()}"
    )


def analisar_sentimento(texto: str, modelo_sentimento: str):
    """Analisa sentimento usando LLM."""
    try:
        resp = client.chat.completions.create(
            model=modelo_sentimento,
            messages=[
                {"role": "system", "content": "Retorne JSON estrito."},
                {"role": "user", "content": _formatar_prompt_sentimento(texto)},
            ],
            temperature=0.0,
            max_tokens=150,
        )
        
        raw = resp.choices[0].message.content.strip()
        
        if raw.startswith("```"):
            raw = re.sub(r'```json\s*|\s*```', '', raw)
        
        data = json.loads(raw)
        
        label = str(data.get("label", "neutro")).lower()
        if label not in {"positivo", "neutro", "negativo"}:
            label = "neutro"
        
        conf = float(data.get("confidence", 0.5))
        conf = max(0.0, min(1.0, conf))
        
        emotions = data.get("emotions", [])
        if not isinstance(emotions, list):
            emotions = [str(emotions)]
        
        reason = str(data.get("reason", "")).strip()
        
        return {
            "label": label,
            "confidence": conf,
            "emotions": [str(e) for e in emotions if str(e).strip()],
            "reason": reason,
        }
        
    except Exception as e:
        return {
            "label": "neutro",
            "confidence": 0.0,
            "emotions": [],
            "reason": f"Falha na anÃ¡lise: {e}",
        }


def _score_from_label(label: str, confidence: float) -> float:
    """Converte rÃ³tulo + confianÃ§a em score âˆˆ [-1, 1]."""
    sgn = 1 if label == "positivo" else (-1 if label == "negativo" else 0)
    c = max(0.0, min(1.0, float(confidence)))
    return round(sgn * c, 3)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOKENIZAÃ‡ÃƒO PT-BR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_PT_STOPWORDS = {
    "a", "Ã ", "Ã s", "ao", "aos", "as", "o", "os", "um", "uma", "uns", "umas",
    "de", "da", "do", "das", "dos", "dÃ¡", "dÃ£o", "em", "no", "na", "nos", "nas",
    "por", "para", "pra", "com", "sem", "entre", "sobre", "sob", "atÃ©", "apÃ³s",
    "que", "se", "Ã©", "ser", "sÃ£o", "era", "eram", "foi", "fui", "vai", "vou",
    "e", "ou", "mas", "como", "quando", "onde", "qual", "quais", "porque", "porquÃª",
    "jÃ¡", "nÃ£o", "sim", "tambÃ©m", "mais", "menos", "muito", "muita", "muitos", "muitas",
    "meu", "minha", "meus", "minhas", "seu", "sua", "seus", "suas",
    "depois", "antes", "este", "esta", "estes", "estas", "isso", "isto",
    "aquele", "aquela", "aqueles", "aquelas", "lhe", "lhes", "ele", "ela", "eles", "elas",
    "vocÃª", "vocÃªs", "nÃ³s", "nosso", "nossa", "nossos", "nossas",
}


def tokenize_pt(texto: str, corrigir: bool = True):
    """Tokeniza texto em PT-BR, remove stopwords e opcionalmente corrige ortografia."""
    if corrigir and CONFIG.get("correcao_ortografica", True):
        texto = corrigir_texto(texto)
    
    texto = texto.lower()
    tokens = re.findall(r'[a-zA-ZÃ€-Ã¿]+', texto)
    tokens = [t for t in tokens if len(t) >= 3 and t not in _PT_STOPWORDS]
    
    return tokens


def gerar_wordcloud(corpus_text: str, width: int = 450, height: int = 280):
    """Gera WordCloud a partir do corpus."""
    if not corpus_text.strip():
        return None, "Digite algo para iniciar a nuvem de palavras."
    
    if not _WORDCLOUD_AVAILABLE:
        return None, "Pacote 'wordcloud' nÃ£o encontrado. Instale: pip install wordcloud"
    
    try:
        wc = WordCloud(
            width=width,
            height=height,
            background_color="white",
            collocations=False,
            max_words=100,
            relative_scaling=0.5,
            min_font_size=8
        )
        wc.generate(corpus_text)
        
        img = wc.to_image()
        buf = BytesIO()
        img.save(buf, format="PNG")
        buf.seek(0)
        
        return buf, None
        
    except Exception as e:
        return None, f"Erro ao gerar wordcloud: {e}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRAFO DE PALAVRAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_word_graph(token_sequences, min_edge_weight: int = 1, max_nodes: int = 500):
    """ConstrÃ³i grafo de coocorrÃªncias com limite de nÃ³s."""
    if not _GRAPH_AVAILABLE:
        return None
    
    G = nx.Graph()
    node_counts = Counter()
    edge_counts = Counter()
    
    for seq in token_sequences:
        node_counts.update(seq)
        for i in range(len(seq) - 1):
            a, b = seq[i], seq[i + 1]
            if a == b:
                continue
            edge = tuple(sorted((a, b)))
            edge_counts[edge] += 1
    
    if len(node_counts) > max_nodes:
        top_words = set([w for w, _ in node_counts.most_common(max_nodes)])
        node_counts = {w: c for w, c in node_counts.items() if w in top_words}
        edge_counts = {
            (a, b): c for (a, b), c in edge_counts.items()
            if a in top_words and b in top_words
        }
    
    for w, c in node_counts.items():
        G.add_node(w, count=int(c))
    
    for (a, b), w in edge_counts.items():
        if w >= max(1, int(min_edge_weight)):
            G.add_edge(a, b, weight=int(w))
    
    return G


def subgraph_paths_to_target(G, target: str, max_depth: int = 4):
    """Extrai subgrafo com caminhos atÃ© o alvo."""
    if G is None or target not in G:
        return None
    
    visited = {target}
    frontier = {target}
    depth = 0
    
    while frontier and depth < max_depth:
        next_frontier = set()
        for u in frontier:
            for v in G.neighbors(u):
                if v not in visited:
                    visited.add(v)
                    next_frontier.add(v)
        frontier = next_frontier
        depth += 1
    
    return G.subgraph(visited).copy()


def render_graph_pyvis(
    G,
    highlight_target: str = None,
    height_px: int = 600,
    dark_mode: bool = False
):
    """Renderiza grafo com PyVis."""
    if not _GRAPH_AVAILABLE or G is None or len(G) == 0:
        return None, "Grafo indisponÃ­vel ou sem dados."
    
    bg = "#0f172a" if dark_mode else "#ffffff"
    fg = "#e5e7eb" if dark_mode else "#333333"
    
    net = Network(
        height=f"{height_px}px",
        width="100%",
        bgcolor=bg,
        font_color=fg,
        notebook=False,
        directed=False,
    )
    
    net.barnes_hut(
        gravity=-2000,
        central_gravity=0.3,
        spring_length=160,
        spring_strength=0.01,
        damping=0.9,
    )
    
    node_counts = nx.get_node_attributes(G, "count")
    max_count = max(node_counts.values()) if node_counts else 1
    
    for node, data in G.nodes(data=True):
        count = int(data.get("count", 1))
        size = 10 + (30 * (count / max_count))
        
        color_high = "#34d399" if dark_mode else "#10b981"
        color_norm = "#93c5fd" if dark_mode else "#60a5fa"
        color = color_high if node == highlight_target else color_norm
        
        title = f"{node}<br/>freq: {count}"
        net.add_node(node, label=node, size=size, color=color, title=title)
    
    for u, v, data in G.edges(data=True):
        w = int(data.get("weight", 1))
        width = 1 + min(10, w)
        title = f"{u} â€” {v}<br/>coocorrÃªncias: {w}"
        net.add_edge(u, v, value=w, width=width, title=title)
    
    return net.generate_html(), None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROCESSAMENTO DE ARQUIVOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def processar_txt(uploaded_file):
    """Processa arquivo .txt"""
    try:
        texto = uploaded_file.read().decode('utf-8')
        return texto, None
    except UnicodeDecodeError:
        try:
            uploaded_file.seek(0)
            texto = uploaded_file.read().decode('latin-1')
            return texto, None
        except Exception as e:
            return None, f"Erro ao decodificar TXT: {e}"


def processar_csv(uploaded_file):
    """Processa arquivo .csv e extrai texto"""
    if not _PANDAS_AVAILABLE:
        return None, "Instale pandas: pip install pandas"
    
    try:
        df = pd.read_csv(uploaded_file)
        
        colunas_possiveis = ['mensagem', 'message', 'texto', 'text', 'content', 'conteudo']
        coluna_msg = None
        
        for col in df.columns:
            if col.lower() in colunas_possiveis:
                coluna_msg = col
                break
        
        if not coluna_msg:
            texto = df.to_string(index=False)
        else:
            texto = '\n'.join(df[coluna_msg].astype(str).tolist())
        
        return texto, None
        
    except Exception as e:
        return None, f"Erro ao processar CSV: {e}"


def processar_docx(uploaded_file):
    """Processa arquivo .docx"""
    try:
        import docx
        doc = docx.Document(uploaded_file)
        texto = '\n'.join([paragrafo.text for paragrafo in doc.paragraphs])
        return texto, None
    except ImportError:
        return None, "Instale python-docx: pip install python-docx"
    except Exception as e:
        return None, f"Erro ao processar DOCX: {e}"


def processar_pdf(uploaded_file):
    """Processa arquivo .pdf"""
    try:
        import PyPDF2
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        texto = ''
        for page in pdf_reader.pages:
            texto += page.extract_text() + '\n'
        return texto, None
    except ImportError:
        return None, "Instale PyPDF2: pip install PyPDF2"
    except Exception as e:
        return None, f"Erro ao processar PDF: {e}"


def analisar_arquivo_importado(texto: str):
    """Analisa texto importado de arquivo externo."""
    if not texto or not texto.strip():
        return None, "Arquivo vazio ou sem texto vÃ¡lido"
    
    if CONFIG.get("correcao_ortografica", True):
        texto_corrigido = corrigir_texto(texto)
    else:
        texto_corrigido = texto
    
    tokens = tokenize_pt(texto_corrigido, corrigir=False)
    
    if not tokens:
        return None, "Nenhuma palavra vÃ¡lida encontrada no arquivo"
    
    linhas = [l.strip() for l in texto_corrigido.split('\n') if l.strip()]
    
    sentimentos = []
    for i, linha in enumerate(linhas[:50]):
        if len(linha) > 10:
            sent = analisar_sentimento(linha, CONFIG["modelo_sentimento"])
            sentimentos.append({
                "linha": i + 1,
                "texto": linha[:100] + "..." if len(linha) > 100 else linha,
                "sentimento": sent
            })
    
    stats = {
        "total_caracteres": len(texto),
        "total_linhas": len(linhas),
        "total_palavras": len(tokens),
        "palavras_unicas": len(set(tokens)),
        "riqueza_vocabular": len(set(tokens)) / len(tokens) * 100 if tokens else 0,
        "sentimentos_analisados": len(sentimentos),
        "top_palavras": Counter(tokens).most_common(20),
    }
    
    if sentimentos:
        scores = [_score_from_label(s["sentimento"]["label"], s["sentimento"]["confidence"]) 
                  for s in sentimentos]
        stats["sentimento_medio"] = sum(scores) / len(scores)
        stats["sentimento_geral"] = (
            "positivo" if stats["sentimento_medio"] > 0.2 else
            "negativo" if stats["sentimento_medio"] < -0.2 else
            "neutro"
        )
    
    return {
        "texto_original": texto,
        "texto_corrigido": texto_corrigido,
        "tokens": tokens,
        "linhas": linhas,
        "sentimentos": sentimentos,
        "stats": stats
    }, None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERSISTÃŠNCIA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def salvar_sessao():
    """Salva estado da sessÃ£o em arquivo."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"sessao_{timestamp}.pkl"
    
    try:
        data = {
            "mensagens": st.session_state.get("lista_mensagens", []),
            "sentiment_history": st.session_state.get("sentiment_history", []),
            "corpus": st.session_state.get("user_corpus_text", ""),
            "tokens": st.session_state.get("user_token_sequences", []),
        }
        
        with open(filename, "wb") as f:
            pickle.dump(data, f)
        
        return filename
        
    except Exception as e:
        st.error(f"Erro ao salvar: {e}")
        return None


def carregar_sessao(uploaded_file):
    """Carrega sessÃ£o de arquivo."""
    try:
        data = pickle.load(uploaded_file)
        
        st.session_state["lista_mensagens"] = data.get("mensagens", [])
        st.session_state["sentiment_history"] = data.get("sentiment_history", [])
        st.session_state["user_corpus_text"] = data.get("corpus", "")
        st.session_state["user_token_sequences"] = data.get("tokens", [])
        
        return True
        
    except Exception as e:
        st.error(f"Erro ao carregar: {e}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAÃ‡ÃƒO DA INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(
    page_title="Assistente de Atendimento",
    page_icon="ğŸ§‘â€ğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ§‘â€ğŸ’¬ Analisador de Conversas com CorreÃ§Ã£o OrtogrÃ¡fica")
st.write("---")
st.caption("â€¢ ğŸ§  Sentimento  â€¢ â˜ï¸ WordCloud  â€¢ ğŸ”— Grafo de Palavras  â€¢ âœï¸ CorreÃ§Ã£o AutomÃ¡tica")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIDEBAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.title("âš™ï¸ PAINEL DE CONTROLE")

# CorreÃ§Ã£o OrtogrÃ¡fica
st.sidebar.write("### âœï¸ CorreÃ§Ã£o OrtogrÃ¡fica")
correcao_habilitada = st.sidebar.toggle(
    "Ativar correÃ§Ã£o automÃ¡tica",
    value=CONFIG.get("correcao_ortografica", True),
    help="Corrige erros de digitaÃ§Ã£o antes da anÃ¡lise"
)
CONFIG["correcao_ortografica"] = correcao_habilitada

if correcao_habilitada:
    st.sidebar.caption("âœ… Palavras serÃ£o corrigidas automaticamente")
else:
    st.sidebar.caption("âš ï¸ Usando texto original (pode ter erros)")

st.sidebar.write("---")

# Sentimento
st.sidebar.write("### ğŸ§  AnÃ¡lise de Sentimento")
sentimento_habilitado = st.sidebar.toggle(
    "Ativar anÃ¡lise de sentimento",
    value=CONFIG.get("sentimento_habilitado", True),
)

sent_container = st.sidebar.container()
sent_container.caption("Ãšltima mensagem do usuÃ¡rio")

# EvoluÃ§Ã£o do Sentimento - GRÃFICO MELHORADO
st.sidebar.write("### ğŸ“ˆ EvoluÃ§Ã£o do Sentimento")
with st.sidebar.container():
    _hist = st.session_state.get("sentiment_history", [])
    if _hist:
        _scores = [h.get("score", 0.0) for h in _hist]
        
        # Cria DataFrame para melhor controle do grÃ¡fico
        if _PANDAS_AVAILABLE:
            df_sent = pd.DataFrame({
                'Mensagem': range(1, len(_scores) + 1),
                'Score': _scores
            })
            
            # GrÃ¡fico de linha com espaÃ§amento reduzido
            st.line_chart(
                df_sent.set_index('Mensagem'),
                height=180,
                use_container_width=True
            )
        else:
            # Fallback sem pandas
            st.line_chart(_scores, height=180, use_container_width=True)
        
        _last = _hist[-1]
        
        # EstatÃ­sticas resumidas
        col_s1, col_s2 = st.sidebar.columns(2)
        with col_s1:
            st.caption(f"**Total:** {len(_scores)}")
        with col_s2:
            st.caption(f"**Ãšltimo:** {_last.get('label', '?')}")
        
        # MÃ©dia e tendÃªncia
        media_score = sum(_scores) / len(_scores)
        tendencia = "â†—ï¸" if len(_scores) > 1 and _scores[-1] > _scores[-2] else "â†˜ï¸" if len(_scores) > 1 and _scores[-1] < _scores[-2] else "â†’"
        
        st.sidebar.caption(f"**MÃ©dia:** {media_score:.2f} {tendencia}")
        
    else:
        st.info("Envie uma mensagem para ver o grÃ¡fico.")

st.sidebar.write("---")

# WordCloud
st.sidebar.write("### â˜ï¸ Nuvem de Palavras")
wc_container = st.sidebar.container()

col_wc1, col_wc2 = st.sidebar.columns(2)
with col_wc1:
    if st.button("ğŸ—‘ï¸ Limpar nuvem", use_container_width=True):
        st.session_state["user_corpus_text"] = ""
        st.session_state["user_token_sequences"] = []
        st.rerun()

st.sidebar.write("---")

# Grafo
st.sidebar.write("### ğŸ”— Grafo de Palavras")
graph_container = st.sidebar.container()

with graph_container:
    min_edge_weight = st.slider(
        "MÃ­n. coocorrÃªncias (aresta)",
        1, 5, 1,
        help="Filtra arestas fracas"
    )
    
    max_path_depth = st.slider(
        "Profundidade mÃ¡x. caminho",
        1, 8, 4,
        help="Caminhos atÃ© a palavra alvo"
    )
    
    show_paths_only = st.toggle(
        "Mostrar apenas caminhos atÃ© palavra alvo",
        value=True
    )
    
    graph_dark_mode = st.toggle(
        "Modo escuro (grafo)",
        value=True
    )

st.sidebar.write("---")

# Gerenciar Dados - NOVO COM TABS
st.sidebar.write("### ğŸ’¾ Gerenciar Dados")

tab_sessao, tab_importar = st.sidebar.tabs(["ğŸ’¾ SessÃ£o", "ğŸ“ Importar"])

with tab_sessao:
    st.caption("Salve/carregue conversas do sistema")
    
    col_save, col_load = st.columns(2)
    
    with col_save:
        if st.button("ğŸ’¾ Salvar", use_container_width=True, key="save_session"):
            filename = salvar_sessao()
            if filename:
                st.success(f"âœ… {filename}")
    
    with col_load:
        uploaded_session = st.file_uploader(
            "Carregar sessÃ£o",
            type=["pkl"],
            label_visibility="collapsed",
            key="upload_session"
        )
        if uploaded_session:
            if carregar_sessao(uploaded_session):
                st.success("âœ… Carregada!")
                st.rerun()

with tab_importar:
    st.caption("Analise arquivos externos")
    
    uploaded_file = st.file_uploader(
        "Upload de arquivo",
        type=["txt", "csv", "pdf", "docx"],
        help="Formatos: TXT, CSV, PDF, DOCX",
        label_visibility="collapsed",
        key="upload_file"
    )
    
    if uploaded_file:
        file_ext = uploaded_file.name.split('.')[-1].lower()
        
        with st.spinner(f"ğŸ“„ Processando {file_ext.upper()}..."):
            # Processa conforme extensÃ£o
            if file_ext == 'txt':
                texto, erro = processar_txt(uploaded_file)
            elif file_ext == 'csv':
                texto, erro = processar_csv(uploaded_file)
            elif file_ext == 'docx':
                texto, erro = processar_docx(uploaded_file)
            elif file_ext == 'pdf':
                texto, erro = processar_pdf(uploaded_file)
            else:
                texto, erro = None, "Formato nÃ£o suportado"
            
            if erro:
                st.error(f"âŒ {erro}")
            elif texto:
                # Analisa o arquivo
                resultado, erro_analise = analisar_arquivo_importado(texto)
                
                if erro_analise:
                    st.error(f"âŒ {erro_analise}")
                else:
                    st.success(f"âœ… Arquivo processado!")
                    
                    # Salva resultado
                    st.session_state["arquivo_importado"] = resultado
                    
                    # OpÃ§Ãµes
                    st.write("**AÃ§Ãµes:**")
                    
                    col_a1, col_a2 = st.columns(2)
                    
                    with col_a1:
                        if st.button("â• Adicionar", use_container_width=True, key="add_file"):
                            # Adiciona ao corpus
                            st.session_state["user_corpus_text"] += " " + " ".join(resultado["tokens"])
                            st.session_state["user_token_sequences"].append(resultado["tokens"])
                            
                            # Adiciona sentimentos
                            for sent_data in resultado["sentimentos"]:
                                sent = sent_data["sentimento"]
                                st.session_state["sentiment_history"].append({
                                    "idx": len(st.session_state["sentiment_history"]) + 1,
                                    "label": sent["label"],
                                    "confidence": sent["confidence"],
                                    "score": _score_from_label(sent["label"], sent["confidence"])
                                })
                            
                            st.success("âœ… Integrado!")
                            time.sleep(1)
                            st.rerun()
                    
                    with col_a2:
                        if st.button("ğŸ“Š Ver", use_container_width=True, key="view_report"):
                            st.session_state["mostrar_relatorio_arquivo"] = True
                            st.rerun()

st.sidebar.write("---")

# AÃ§Ãµes
st.sidebar.write("### ğŸ› ï¸ AÃ§Ãµes")

col1, col2 = st.sidebar.columns(2)
with col1:
    if st.button("ğŸ—‘ï¸ Limpar chat", use_container_width=True):
        st.session_state["lista_mensagens"] = []
        st.session_state["sentimento_atual"] = None
        st.session_state["user_corpus_text"] = ""
        st.session_state["user_token_sequences"] = []
        st.session_state["sentiment_history"] = []
        st.rerun()

with col2:
    if st.button("ğŸ”„ Recarregar", use_container_width=True):
        st.cache_data.clear()
        st.rerun()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ESTADO DA APLICAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if "lista_mensagens" not in st.session_state:
    st.session_state["lista_mensagens"] = []

if "sentimento_atual" not in st.session_state:
    st.session_state["sentimento_atual"] = None

if "user_corpus_text" not in st.session_state:
    st.session_state["user_corpus_text"] = ""

if "user_token_sequences" not in st.session_state:
    st.session_state["user_token_sequences"] = []

if "sentiment_history" not in st.session_state:
    st.session_state["sentiment_history"] = []

if "grafo_html" not in st.session_state:
    st.session_state["grafo_html"] = ""

if "_rerun_flag" not in st.session_state:
    st.session_state["_rerun_flag"] = False

if "arquivo_importado" not in st.session_state:
    st.session_state["arquivo_importado"] = None

if "mostrar_relatorio_arquivo" not in st.session_state:
    st.session_state["mostrar_relatorio_arquivo"] = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RENDERIZAÃ‡ÃƒO DO HISTÃ“RICO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

for msg in st.session_state["lista_mensagens"]:
    if msg["role"] == "user":
        st.chat_message("user").write(msg["content"])
    elif msg["role"] == "assistant":
        st.chat_message("assistant").write(msg["content"])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENTRADA DO USUÃRIO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

mensagem_usuario = st.chat_input("ğŸ’­ Digite sua mensagem aqui...")

if mensagem_usuario:
    # Mostra mensagem original
    st.chat_message("user").write(mensagem_usuario)
    
    # CorreÃ§Ã£o ortogrÃ¡fica
    if correcao_habilitada:
        texto_corrigido = corrigir_texto(mensagem_usuario)
        if texto_corrigido != mensagem_usuario:
            with st.expander("âœï¸ Texto corrigido automaticamente"):
                col_antes, col_depois = st.columns(2)
                with col_antes:
                    st.caption("**Original:**")
                    st.text(mensagem_usuario)
                with col_depois:
                    st.caption("**Corrigido:**")
                    st.text(texto_corrigido)
    else:
        texto_corrigido = mensagem_usuario
    
    # Adiciona ao histÃ³rico
    st.session_state["lista_mensagens"].append(
        {"role": "user", "content": texto_corrigido}
    )
    
    # Tokeniza
    tokens = tokenize_pt(texto_corrigido, corrigir=False)
    
    if tokens:
        st.session_state["user_corpus_text"] += " " + " ".join(tokens)
        st.session_state["user_token_sequences"].append(tokens)
    
    # AnÃ¡lise de Sentimento
    if sentimento_habilitado:
        with st.spinner("ğŸ§  Analisando sentimento..."):
            resultado_sentimento = analisar_sentimento(
                texto_corrigido,
                modelo_sentimento=CONFIG["modelo_sentimento"]
            )
            st.session_state["sentimento_atual"] = resultado_sentimento
            
            idx_user = sum(
                1 for m in st.session_state["lista_mensagens"]
                if m.get("role") == "user"
            )
            
            st.session_state["sentiment_history"].append({
                "idx": idx_user,
                "label": resultado_sentimento.get("label", "neutro"),
                "confidence": float(resultado_sentimento.get("confidence", 0.0)),
                "score": _score_from_label(
                    resultado_sentimento.get("label", "neutro"),
                    float(resultado_sentimento.get("confidence", 0.0))
                ),
            })
    
    # Resposta do Assistente
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” Pensando na resposta..."):
            progress_bar = st.progress(0)
            for i in range(100):
                time.sleep(0.01)
                progress_bar.progress(i + 1)
            progress_bar.empty()
            
            try:
                resposta = client.chat.completions.create(
                    model=CONFIG["modelo_padrao"],
                    messages=obter_mensagens_completas(),
                    temperature=CONFIG["temperatura_padrao"],
                    max_tokens=CONFIG["max_tokens_padrao"],
                    top_p=0.9,
                    frequency_penalty=0.1,
                )
                
                resposta_ia = resposta.choices[0].message.content or ""
                st.write(resposta_ia)
                
                st.session_state["lista_mensagens"].append(
                    {"role": "assistant", "content": resposta_ia}
                )
                
                # Recarrega visualizaÃ§Ãµes
                if not st.session_state.get("_rerun_flag"):
                    st.session_state["_rerun_flag"] = True
                    st.rerun()
                else:
                    st.session_state["_rerun_flag"] = False
                
            except Exception as e:
                st.error(f"âŒ Erro na API: {str(e)}")
                st.info("ğŸ’¡ Verifique sua chave API e conexÃ£o.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIDEBAR: VISUALIZAÃ‡Ã•ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _badge(label: str) -> str:
    """Cria badge colorido para o sentimento."""
    colors = {
        "positivo": "#16a34a",
        "neutro": "#6b7280",
        "negativo": "#dc2626"
    }
    color = colors.get(label, "#6b7280")
    return (
        f"<span style='background:{color};color:white;padding:4px 10px;"
        f"border-radius:999px;font-weight:600;font-size:12px;'>"
        f"{label.upper()}</span>"
    )


# Sentimento
with sent_container:
    data = st.session_state.get("sentimento_atual")
    
    if sentimento_habilitado and data:
        st.markdown(_badge(data["label"]), unsafe_allow_html=True)
        st.metric("ConfianÃ§a", f"{round(data['confidence'] * 100):d}%")
        
        if data["emotions"]:
            emotes = " ".join([f"`{e}`" for e in data["emotions"][:6]])
            st.write(f"**EmoÃ§Ãµes:** {emotes}")
        
        if data.get("reason"):
            with st.expander("ğŸ“ Justificativa"):
                st.write(data["reason"])
    
    elif sentimento_habilitado:
        st.info("Envie uma mensagem para ver o sentimento.")


# WordCloud
with wc_container:
    corpus = st.session_state.get("user_corpus_text", "")
    
    if corpus.strip():
        buf, err = gerar_wordcloud(corpus)
        
        if err:
            st.warning(err)
        elif buf:
            st.image(buf, caption="Nuvem de Palavras (Corrigidas)", use_container_width=True)
            
            st.download_button(
                "ğŸ“¥ Baixar PNG",
                data=buf,
                file_name=f"wordcloud_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                mime="image/png",
                use_container_width=True,
            )
            
            tokens_unicos = len(set(corpus.split()))
            tokens_totais = len(corpus.split())
            st.caption(f"ğŸ“Š {tokens_totais} palavras | {tokens_unicos} Ãºnicas")
    else:
        st.info("Digite mensagens para gerar a nuvem.")


# Grafo
with graph_container:
    token_seqs = st.session_state.get("user_token_sequences", [])
    
    if not _GRAPH_AVAILABLE:
        st.info("Instale: pip install networkx pyvis")
    
    elif len(token_seqs) == 0:
        st.info("Envie mensagens para gerar o grafo.")
    
    else:
        with st.spinner("ğŸ”— Construindo grafo..."):
            G_full = build_word_graph(
                token_seqs,
                min_edge_weight=min_edge_weight,
                max_nodes=500
            )
        
        if G_full is None or len(G_full) == 0:
            st.warning("Grafo vazio. Envie mais mensagens.")
        
        else:
            if len(G_full.nodes()) >= 500:
                st.warning("âš ï¸ Mostrando top 500 palavras.")
            
            counts = nx.get_node_attributes(G_full, "count")
            words_sorted = sorted(counts.items(), key=lambda x: (-x[1], x[0]))
            top_words = [w for w, c in words_sorted[:200]]
            
            target = st.selectbox(
                "ğŸ¯ Palavra alvo:",
                options=["(nenhuma)"] + top_words,
                help="Destaca palavra no grafo"
            )
            
            G_view = G_full
            
            if show_paths_only and target and target != "(nenhuma)":
                G_tmp = subgraph_paths_to_target(G_full, target, max_depth=max_path_depth)
                
                if G_tmp is not None and len(G_tmp) > 0:
                    G_view = G_tmp
                    st.caption(f"ğŸ” {len(G_view.nodes())} nÃ³s conectados a '{target}'")
                else:
                    st.info(f"Sem caminhos para '{target}'")
                    G_view = None
            
            if G_view is not None and len(G_view) > 0:
                html, gerr = render_graph_pyvis(
                    G_view,
                    highlight_target=target if target != "(nenhuma)" else None,
                    height_px=520,
                    dark_mode=graph_dark_mode
                )
                
                if gerr:
                    st.error(gerr)
                else:
                    st.session_state["grafo_html"] = html
                    
                    st.components.v1.html(html, height=540, scrolling=True)
                    
                    st.caption(
                        f"ğŸ“Š {len(G_view.nodes())} nÃ³s | "
                        f"{len(G_view.edges())} arestas | "
                        f"Densidade: {nx.density(G_view):.3f}"
                    )
                    
                    col_g1, col_g2 = st.sidebar.columns(2)
                    
                    with col_g1:
                        if st.button("ğŸ“± Expandir", use_container_width=True, key="expand_graph"):
                            st.session_state["grafo_expand_main"] = True
                            st.rerun()
                    
                    with col_g2:
                        st.download_button(
                            "ğŸ“¥ HTML",
                            data=html,
                            file_name=f"grafo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                            mime="text/html",
                            use_container_width=True,
                        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRAFO EXPANDIDO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if st.session_state.get("grafo_expand_main") and st.session_state.get("grafo_html"):
    st.markdown("---")
    st.markdown("## ğŸ”— Grafo de Palavras (VisualizaÃ§Ã£o Expandida)")
    
    st_html(st.session_state["grafo_html"], height=820, scrolling=True)
    
    col_exp1, col_exp2, col_exp3 = st.columns(3)
    
    with col_exp1:
        if st.button("â†©ï¸ Recolher", use_container_width=True):
            st.session_state["grafo_expand_main"] = False
            st.rerun()
    
    with col_exp2:
        st.download_button(
            "ğŸ“¥ Baixar HTML",
            data=st.session_state["grafo_html"],
            file_name=f"grafo_expandido_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
            mime="text/html",
            use_container_width=True,
        )
    
    with col_exp3:
        if _GRAPH_AVAILABLE:
            token_seqs = st.session_state.get("user_token_sequences", [])
            total_palavras = sum(len(seq) for seq in token_seqs)
            st.metric("Total Palavras", total_palavras)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RELATÃ“RIO DE ARQUIVO IMPORTADO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if st.session_state.get("mostrar_relatorio_arquivo") and st.session_state.get("arquivo_importado"):
    st.markdown("---")
    st.markdown("## ğŸ“Š AnÃ¡lise do Arquivo Importado")
    
    resultado = st.session_state["arquivo_importado"]
    stats = resultado["stats"]
    
    # MÃ©tricas principais
    col_m1, col_m2, col_m3, col_m4 = st.columns(4)
    
    with col_m1:
        st.metric("ğŸ“ Total Palavras", f"{stats['total_palavras']:,}")
    
    with col_m2:
        st.metric("ğŸ”¤ Palavras Ãšnicas", f"{stats['palavras_unicas']:,}")
    
    with col_m3:
        st.metric("ğŸ“ˆ Riqueza Vocabular", f"{stats['riqueza_vocabular']:.1f}%")
    
    with col_m4:
        if "sentimento_geral" in stats:
            st.metric(
                "ğŸ­ Sentimento Geral",
                stats["sentimento_geral"].capitalize(),
                delta=f"{stats['sentimento_medio']:.2f}"
            )
    
    # Tabs com visualizaÃ§Ãµes
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š EstatÃ­sticas", "ğŸ§  Sentimentos", "â˜ï¸ WordCloud", "ğŸ“„ Texto"])
    
    with tab1:
        st.write("### Top 20 Palavras Mais Frequentes")
        
        if _PANDAS_AVAILABLE:
            palavras_freq = stats["top_palavras"]
            df_palavras = pd.DataFrame(palavras_freq, columns=["Palavra", "FrequÃªncia"])
            
            st.bar_chart(df_palavras.set_index("Palavra"))
            st.dataframe(df_palavras, use_container_width=True)
        else:
            for palavra, freq in stats["top_palavras"]:
                st.write(f"**{palavra}**: {freq} vezes")
    
    with tab2:
        st.write("### AnÃ¡lise de Sentimento por Segmento")
        
        if resultado["sentimentos"]:
            for sent_data in resultado["sentimentos"][:20]:
                sent = sent_data["sentimento"]
                
                col_s1, col_s2 = st.columns([3, 1])
                
                with col_s1:
                    st.write(f"**Linha {sent_data['linha']}:** {sent_data['texto']}")
                
                with col_s2:
                    st.markdown(_badge(sent["label"]), unsafe_allow_html=True)
                    st.caption(f"Conf: {int(sent['confidence']*100)}%")
                
                st.divider()
        else:
            st.info("Nenhum segmento analisado")
    
    with tab3:
        st.write("### Nuvem de Palavras do Arquivo")
        
        corpus_arquivo = " ".join(resultado["tokens"])
        buf, err = gerar_wordcloud(corpus_arquivo, width=800, height=500)
        
        if err:
            st.warning(err)
        elif buf:
            st.image(buf, use_container_width=True)
            
            st.download_button(
                "ğŸ“¥ Baixar WordCloud",
                data=buf,
                file_name=f"wordcloud_arquivo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                mime="image/png",
            )
    
    with tab4:
        st.write("### Texto Original vs Corrigido")
        
        col_t1, col_t2 = st.columns(2)
        
        with col_t1:
            st.write("**Original:**")
            st.text_area(
                "original",
                resultado["texto_original"][:5000],
                height=400,
                label_visibility="collapsed"
            )
        
        with col_t2:
            st.write("**Corrigido:**")
            st.text_area(
                "corrigido",
                resultado["texto_corrigido"][:5000],
                height=400,
                label_visibility="collapsed"
            )
    
    # BotÃ£o fechar
    if st.button("âœ–ï¸ Fechar RelatÃ³rio"):
        st.session_state["mostrar_relatorio_arquivo"] = False
        st.rerun()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPORTAÃ‡ÃƒO DE RELATÃ“RIO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.markdown("---")
st.subheader("ğŸ“Š Exportar RelatÃ³rio de AnÃ¡lise")

col_report1, col_report2 = st.columns(2)

with col_report1:
    if st.button("ğŸ“„ Gerar RelatÃ³rio TXT", use_container_width=True):
        relatorio = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RELATÃ“RIO DE ANÃLISE DE CONVERSAS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Data: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
Modelo: {CONFIG['modelo_padrao']}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ESTATÃSTICAS GERAIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Total de Mensagens: {len(st.session_state.get('lista_mensagens', []))}
Mensagens do UsuÃ¡rio: {sum(1 for m in st.session_state.get('lista_mensagens', []) if m['role'] == 'user')}
Mensagens do Assistente: {sum(1 for m in st.session_state.get('lista_mensagens', []) if m['role'] == 'assistant')}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANÃLISE DE SENTIMENTO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
        
        hist = st.session_state.get("sentiment_history", [])
        if hist:
            positivos = sum(1 for h in hist if h["label"] == "positivo")
            neutros = sum(1 for h in hist if h["label"] == "neutro")
            negativos = sum(1 for h in hist if h["label"] == "negativo")
            
            relatorio += f"""
Mensagens Positivas: {positivos} ({positivos/len(hist)*100:.1f}%)
Mensagens Neutras: {neutros} ({neutros/len(hist)*100:.1f}%)
Mensagens Negativas: {negativos} ({negativos/len(hist)*100:.1f}%)

Score MÃ©dio: {sum(h['score'] for h in hist)/len(hist):.3f}
ConfianÃ§a MÃ©dia: {sum(h['confidence'] for h in hist)/len(hist)*100:.1f}%
"""
        else:
            relatorio += "\nNenhuma anÃ¡lise disponÃ­vel.\n"
        
        relatorio += f"""
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANÃLISE DE VOCABULÃRIO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
        
        corpus = st.session_state.get("user_corpus_text", "")
        if corpus:
            tokens = corpus.split()
            palavras_unicas = set(tokens)
            
            relatorio += f"""
Total de Palavras: {len(tokens)}
Palavras Ãšnicas: {len(palavras_unicas)}
Riqueza Vocabular: {len(palavras_unicas)/len(tokens)*100:.1f}%

Top 10 Palavras:
"""
            counter = Counter(tokens)
            for palavra, freq in counter.most_common(10):
                relatorio += f"  {palavra}: {freq} vezes\n"
        
        relatorio += f"""
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HISTÃ“RICO DE MENSAGENS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
        
        for i, msg in enumerate(st.session_state.get("lista_mensagens", []), 1):
            role = "USUÃRIO" if msg["role"] == "user" else "ASSISTENTE"
            relatorio += f"\n[{i}] {role}:\n{msg['content']}\n"
        
        relatorio += "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        
        st.download_button(
            "ğŸ“¥ Baixar RelatÃ³rio (.txt)",
            data=relatorio,
            file_name=f"relatorio_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            use_container_width=True,
        )

with col_report2:
    if st.button("ğŸ“Š Gerar RelatÃ³rio JSON", use_container_width=True):
        relatorio_json = {
            "metadata": {
                "data_geracao": datetime.now().isoformat(),
                "modelo": CONFIG["modelo_padrao"],
                "temperatura": CONFIG["temperatura_padrao"],
                "correcao_ortografica": correcao_habilitada,
            },
            "estatisticas": {
                "total_mensagens": len(st.session_state.get("lista_mensagens", [])),
                "mensagens_usuario": sum(1 for m in st.session_state.get("lista_mensagens", []) if m["role"] == "user"),
                "mensagens_assistente": sum(1 for m in st.session_state.get("lista_mensagens", []) if m["role"] == "assistant"),
            },
            "sentimento": {
                "historico": st.session_state.get("sentiment_history", []),
            },
            "vocabulario": {
                "corpus": st.session_state.get("user_corpus_text", ""),
                "sequencias_tokens": st.session_state.get("user_token_sequences", []),
            },
            "mensagens": st.session_state.get("lista_mensagens", []),
        }
        
        json_str = json.dumps(relatorio_json, ensure_ascii=False, indent=2)
        
        st.download_button(
            "ğŸ“¥ Baixar RelatÃ³rio (.json)",
            data=json_str,
            file_name=f"relatorio_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json",
            use_container_width=True,
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RODAPÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.markdown("---")

col_info1, col_info2, col_info3 = st.columns(3)

with col_info1:
    st.caption(f"**Modelo:** {CONFIG['modelo_padrao']}")
    st.caption(f"**Temperatura:** {CONFIG['temperatura_padrao']}")

with col_info2:
    total_msgs = len(st.session_state.get("lista_mensagens", []))
    msgs_user = sum(1 for m in st.session_state.get("lista_mensagens", []) if m["role"] == "user")
    st.caption(f"**Mensagens:** {total_msgs} ({msgs_user} do usuÃ¡rio)")

with col_info3:
    if correcao_habilitada:
        st.caption("âœ… **CorreÃ§Ã£o:** Ativa")
    else:
        st.caption("âš ï¸ **CorreÃ§Ã£o:** Desativada")

st.caption(
    "ğŸ¤– Assistente de Atendimento com IA | "
    f"Powered by OpenAI | VersÃ£o 2.1 | {datetime.now().strftime('%Y')}"
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AJUDA E DICAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.expander("ğŸ’¡ Dicas de Uso"):
    st.markdown("""
    ### Como usar este assistente:
    
    **ğŸ”§ CorreÃ§Ã£o OrtogrÃ¡fica**
    - Ative na sidebar para corrigir automaticamente erros
    - CorreÃ§Ãµes: "vc" â†’ "vocÃª", "nao" â†’ "nÃ£o", etc.
    - Texto corrigido usado em todas as anÃ¡lises
    
    **ğŸ“ Importar Arquivos**
    - Formatos aceitos: TXT, CSV, PDF, DOCX
    - CSV: detecta automaticamente coluna de mensagens
    - OpÃ§Ãµes: adicionar ao chat atual ou ver relatÃ³rio separado
    
    **ğŸ§  AnÃ¡lise de Sentimento**
    - Verde = Positivo | Cinza = Neutro | Vermelho = Negativo
    - GrÃ¡fico mostra evoluÃ§Ã£o em tempo real
    - EspaÃ§amento reduzido para visualizar mudanÃ§as
    
    **â˜ï¸ Nuvem de Palavras**
    - Palavras maiores = mais frequentes
    - Apenas palavras corrigidas sÃ£o exibidas
    
    **ğŸ”— Grafo de Palavras**
    - Mostra coocorrÃªncias (palavras que aparecem juntas)
    - Use "Palavra alvo" para focar em conexÃµes especÃ­ficas
    - Limite de 500 nÃ³s para melhor performance
    
    **ğŸ’¾ Salvar/Carregar**
    - Salve sessÃµes completas (.pkl)
    - Carregue sessÃµes anteriores para continuar
    
    **ğŸ“Š RelatÃ³rios**
    - TXT: relatÃ³rio formatado para leitura
    - JSON: dados estruturados para anÃ¡lise externa
    """)

with st.expander("âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas"):
    st.markdown(f"""
    ### ParÃ¢metros do Sistema:
    
    **Modelo:** `{CONFIG['modelo_padrao']}`
    - Modelo OpenAI usado para respostas
    - gpt-4o-mini = rÃ¡pido e econÃ´mico
    - gpt-4o = mais preciso e contextual
    
    **Temperatura:** `{CONFIG['temperatura_padrao']}`
    - Controla criatividade (0.0 a 1.0)
    - Baixo = mais determinÃ­stico
    - Alto = mais variado
    
    **Contexto:** `{CONFIG['max_contexto_mensagens']}` mensagens
    - Quantas mensagens sÃ£o enviadas Ã  API
    - Mais contexto = melhor memÃ³ria, mais custo
    
    **CorreÃ§Ã£o OrtogrÃ¡fica:**
    - DicionÃ¡rio: {len(CORREÃ‡Ã•ES_ORTOGRÃFICAS)} correÃ§Ãµes
    - Processamento local (sem API)
    - Preserva capitalizaÃ§Ã£o original
    
    **Formatos Suportados:**
    - TXT: texto puro (UTF-8 ou Latin-1)
    - CSV: colunas automÃ¡ticas ou manual
    - PDF: extraÃ§Ã£o de texto (PyPDF2)
    - DOCX: parÃ¡grafos do Word (python-docx)
    """)

with st.expander("ğŸ”§ DependÃªncias e InstalaÃ§Ã£o"):
    st.markdown("""
    ### Pacotes NecessÃ¡rios:
    
    **ObrigatÃ³rios:**
```bash
    pip install streamlit openai python-dotenv
```
    
    **Opcionais (para recursos extras):**
```bash
    # Para WordCloud
    pip install wordcloud
    
    # Para Grafo de Palavras
    pip install networkx pyvis
    
    # Para anÃ¡lise de arquivos
    pip install pandas PyPDF2 python-docx
```
    
    **Arquivo .env:**
```
    OPENAI_API_KEY=sk-sua-chave-aqui
    OPENAI_MODEL=gpt-4o-mini
    OPENAI_SENTIMENT_MODEL=gpt-4o-mini
```
    
    **Executar:**
```bash
    streamlit run app.py
```
    """)